{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Â© 2020 Nokia\n",
    "\n",
    "Licensed under the BSD 3 Clause license\n",
    "\n",
    "SPDX-License-Identifier: BSD-3-Clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from fastai.basic_data import DataBunch\n",
    "from fastai.basic_train import Callback, Learner\n",
    "from fastai.callbacks import SaveModelCallback\n",
    "\n",
    "from codesearch.utils import load_model, get_best_device, Saveable\n",
    "from codesearch.encoders import BasicEncoder\n",
    "from codesearch.data import load_snippet_collection, EVAL_DATASETS, eval_datasets_from_regex\n",
    "from codesearch.data_config import CODE_FIELD, LANGUAGE_FIELD, DESCRIPTION_FIELD\n",
    "from codesearch.embedding_retrieval import EmbeddingRetrievalModel\n",
    "from codesearch.unif.unif_embedder import UNIFEmbedder\n",
    "from codesearch.evaluation import evaluate_and_dump \n",
    "from codesearch.unif.unif_modules import SimilarityModel, MarginRankingLoss\n",
    "from codesearch.unif.unif_preprocessing import Padder\n",
    "\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Read configuration paramaters from environment variables (when this notebook is run as a script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# train_snippets_collection = os.environ.get(\"train_snippet_collection\", \"so-ds-feb20\")\n",
    "# snippets_collection = os.environ.get(\"snippet_collection\", \"so-ds-feb20\")\n",
    "# valid_dataset = os.environ.get(\"valid_dataset\", \"so-ds-feb20-valid\")\n",
    "# test_dataset = os.environ.get(\"test_dataset\", \"so-ds-feb20-test\")\n",
    "# ncs_embedder = os.environ.get(\"ncs_embedder\", \"../ncs/so-ds-feb20/best_ncs_embedder/\")\n",
    "# output_dir = os.environ.get(\"output_dir\", \"so-ds-feb20\")\n",
    "\n",
    "# train_snippets_collection = os.environ.get(\"train_snippet_collection\", \"conala-curated\")\n",
    "# snippets_collection = os.environ.get(\"snippet_collection\", \"conala-curated\")\n",
    "# valid_dataset = os.environ.get(\"valid_dataset\", \"conala-curated-0.5-test\")\n",
    "# test_dataset = os.environ.get(\"test_dataset\", \"conala-curated-0.5-test\")\n",
    "# ncs_embedder = os.environ.get(\"ncs_embedder\", \"../ncs/conala/best_ncs_embedder/\")\n",
    "# output_dir = os.environ.get(\"output_dir\", \"conala\")\n",
    "\n",
    "train_snippets_collection = os.environ.get(\"train_snippet_collection\", \"staqc-py-cleaned\")\n",
    "snippets_collection = os.environ.get(\"snippet_collection\", \"staqc-py-cleaned\")\n",
    "valid_dataset = os.environ.get(\"valid_dataset\", \"staqc-py-raw-valid\")\n",
    "test_dataset = os.environ.get(\"test_dataset\", \"staqc-py-raw-test\")\n",
    "ncs_embedder = os.environ.get(\"ncs_embedder\", \"../ncs/staqc-py/best_ncs_embedder/\")\n",
    "output_dir = os.environ.get(\"output_dir\", \"staqc-py\")\n",
    "\n",
    "if not Path(output_dir).exists():\n",
    "    Path(output_dir).mkdir()\n",
    "margin = float(os.environ.get(\"margin\", 0.05))\n",
    "random_init = bool(os.environ.get(\"random_init\", False))\n",
    "\n",
    "momentum = float(os.environ.get(\"momentum\", 0.9))\n",
    "lr = float(os.environ.get(\"lr\", 0.001))\n",
    "epochs = int(os.environ.get(\"epochs\", 20))\n",
    "fit_one_cyle = bool(os.environ.get(\"fit_one_cyle\", False))\n",
    "clip = float(os.environ.get(\"clip\", 0.))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ncs_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "margin, random_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "lr, epochs, momentum, fit_one_cyle, clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "if valid_dataset and valid_dataset not in EVAL_DATASETS:\n",
    "    raise ValueError()\n",
    "test_datasets = eval_datasets_from_regex(test_dataset)\n",
    "snippets = load_snippet_collection(snippets_collection)\n",
    "train_snippets = load_snippet_collection(train_snippets_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "snippets_collection, train_snippets_collection, valid_dataset, test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ncs = load_model(ncs_embedder)\n",
    "ft_model = ncs._ft_model\n",
    "enc =  ncs._enc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "TODO: bucketize the minimize padding in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class CodeSnippetsAndDescriptions(Dataset):\n",
    "    \n",
    "    def __init__(self, snippet_collection, transform=None, deterministic=False, dummy=False):\n",
    "        snippets = load_snippet_collection(snippet_collection)\n",
    "        snippets = [{\"code\": s[CODE_FIELD], \"description\": s[DESCRIPTION_FIELD], \"language\": s[LANGUAGE_FIELD]}\n",
    "                    for s in snippets]\n",
    "        if dummy:\n",
    "            snippets = snippets[:50]\n",
    "        self.snippets = pd.DataFrame(snippets)\n",
    "        self.transform = transform\n",
    "        if deterministic:\n",
    "            random.seed(42)\n",
    "            random_idx = list(range(len(snippets)))\n",
    "            random.shuffle(random_idx)\n",
    "            self.random_idx = np.array(random_idx)\n",
    "        else:\n",
    "            self.random_idx = None\n",
    "    \n",
    "    def random(self, idx):\n",
    "        if self.random_idx is not None:\n",
    "            return self.random_idx[idx]\n",
    "        return random.randint(0, len(self) - 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.snippets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        s = self.snippets.iloc[idx]\n",
    "        code, description, language = s[\"code\"], s[\"description\"], s[\"language\"]        \n",
    "        idx_rand = self.random(idx)\n",
    "        while self.snippets.iloc[idx_rand][\"description\"] == description:\n",
    "            idx_rand = random.randint(0, len(self) - 1)\n",
    "        random_description = self.snippets.iloc[idx_rand][\"description\"]\n",
    "        \n",
    "        x = {\n",
    "            \"code\": code, \n",
    "            \"descriptions\": np.array([description, random_description]), \n",
    "            \"language\": language\n",
    "        }\n",
    "        y = torch.tensor([1, 0], dtype=torch.long, device=get_best_device())\n",
    "        if self.transform:\n",
    "            x, y = self.transform((x, y))\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class Preprocess(object):\n",
    "    \n",
    "    def __init__(self, encoder):\n",
    "        self.encoder = encoder\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        x, y = sample\n",
    "        code, _ = self.encoder.encode_code(x[\"code\"], x[\"language\"])\n",
    "        descriptions = [self.encoder.encode_description(descr) for descr in x[\"descriptions\"]]\n",
    "        x = {\"code\": code, \"descriptions\": descriptions,  \"language\": x[\"language\"]}\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Retrieval model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs, max_code_len=200, max_description_len=25, ft_model=None):\n",
    "    padder = Padder(max_code_len, max_code_len, ft_model=ft_model)\n",
    "    return (\n",
    "        padder,\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True, pin_memory=False, collate_fn=padder),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2, collate_fn=padder, pin_memory=False) if valid_ds else None\n",
    "    )\n",
    "\n",
    "def get_model(ft_model, random_init=False):\n",
    "    model = SimilarityModel(ft_model, random_init=random_init)\n",
    "    return model\n",
    "\n",
    "def create_retrieval_model(model, encoder, snippets, ft_model=None):\n",
    "    unif_embedder = UNIFEmbedder(model, encoder, ft_model, batch_size=2, max_code_len=200, max_description_len=25)\n",
    "    retrieval_model = EmbeddingRetrievalModel(unif_embedder)\n",
    "    retrieval_model.add_snippets(snippets)\n",
    "    return retrieval_model\n",
    "\n",
    "def eval_retrieval(model, encoder, snippets, valid_dataset, test_datasets, ft_model=None):\n",
    "    retrieval_model = create_retrieval_model(model, encoder, snippets, ft_model=ft_model)\n",
    "    results = evaluate_and_dump(retrieval_model, {}, output_dir, valid_dataset, test_datasets)\n",
    "    print(results)\n",
    "    return results[valid_dataset][\"mrr\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_ds = CodeSnippetsAndDescriptions(train_snippets_collection, transform=Preprocess(enc))\n",
    "# we use the retrieval model for validation, this is only a dummy set\n",
    "valid_ds = CodeSnippetsAndDescriptions(train_snippets_collection, transform=Preprocess(enc), deterministic=True, dummy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class MRR(Callback):\n",
    "    \"Wrap a `func` in a callback for metrics computation.\"\n",
    "    def __init__(self, encoder, snippets, valid_dataset, test_datasets, ft_model, model):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.snippets = pd.DataFrame(snippets)\n",
    "        self.ft_model = ft_model\n",
    "        self.model = model\n",
    "        eval_retrieval_fn = partial(eval_retrieval, \n",
    "                            encoder=encoder,\n",
    "                            snippets=snippets, \n",
    "                            valid_dataset=valid_dataset,\n",
    "                            test_datasets=test_datasets,\n",
    "                            ft_model=ft_model\n",
    "                           )\n",
    "        self.name = \"mrr\"\n",
    "        self.func = eval_retrieval_fn\n",
    "        self.model = model\n",
    "        self.best_result = 0\n",
    "        self.epoch = 0\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        \"Set the final result in `last_metrics`.\"\n",
    "        result = self.func(self.model)\n",
    "        print(result, self.best_result)\n",
    "        print(type(result), type(self.best_result))\n",
    "        if result > self.best_result:\n",
    "            print(\"saving model\")\n",
    "            self.model.save(output_dir + f\"/model-epoch={self.epoch}\")\n",
    "            self.best_result = result\n",
    "        self.epoch += 1\n",
    "        return {'last_metrics': last_metrics + [result]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "initial observations:\n",
    "\n",
    "- higher margin does not help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = get_model(ft_model, random_init)\n",
    "collate_fn, train_dl, valid_ds = get_data(train_ds, valid_ds, 32, ft_model=ft_model)\n",
    "db = DataBunch(train_dl, valid_ds, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "loss_func = MarginRankingLoss(margin)\n",
    "opt_func = partial(optim.Adam, betas=(momentum, 0.999))\n",
    "learner = Learner(db, model, loss_func=loss_func, opt_func=opt_func, wd=0, metrics=[MRR(enc, snippets, valid_dataset, test_datasets, ft_model, model)])\n",
    "if clip:\n",
    "    learner.clip_grad(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "learner.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "learner.unfreeze()\n",
    "if fit_one_cyle:\n",
    "    learner.fit_one_cyle(epochs)\n",
    "else:\n",
    "    learner.fit(epochs, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Save best retrieval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# change \"model-epoch=17\" with the best checkpoint\n",
    "best_model = Saveable.load(f\"{output_dir}/model-epoch=14\")\n",
    "#retrieval_model = create_retrieval_model(best_model, enc, snippets, ft_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "unif_embedder = UNIFEmbedder(best_model, enc, ft_model, batch_size=32, max_code_len=200, max_description_len=25)\n",
    "unif_embedder.save(f\"{output_dir}/best_unif_embedder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "config = {\"model\": \"unif_best\"} \n",
    "evaluate_and_dump(retrieval_model, config, output_dir, valid_dataset, test_datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codesearch_ml4",
   "language": "python",
   "name": "codesearch_ml4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
